{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"d2l 3.4.ipynb","provenance":[],"authorship_tag":"ABX9TyPv1DxmdZ6roykwzjGTCg4c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FxB5jtw9ORHv"},"source":["$$\n","-\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})\n","= \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}),\n","$$\n","\n","$$\n","\\begin{aligned}\n","l(\\mathbf{y}, \\hat{\\mathbf{y}}) &=  - \\sum_{j=1}^q y_j \\log \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} \\\\\n","&= \\sum_{j=1}^q y_j \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j\\\\\n","&= \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j.\n","\\end{aligned}\n","$$\n","\n","\n","$$\n","\\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j.\n","$$"]},{"cell_type":"markdown","metadata":{"id":"Ikx2UZKuOhnC"},"source":["### Information Theory\n","- Entropy?\n","- Surprisal?\n","- Cross-Entropy?"]}]}